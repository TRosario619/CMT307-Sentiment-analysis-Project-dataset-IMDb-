{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "id": "99lhtukS6VRv",
    "outputId": "6040e0c0-fdfa-4331-8dc2-35e784eb0140",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ZIqVKTjjsEfx",
    "outputId": "f7353147-8dbc-4948-cc30-1b580cfbe5fc"
   },
   "outputs": [],
   "source": [
    "url='C:/Users/tassi/Downloads/CMT307/Coursework1/datasets_coursework1/IMDb/'\n",
    "\n",
    "#Load positive reviews train\n",
    "path= url+'train/imdb_train_pos.txt'\n",
    "df_train_pos=pd.read_csv(path,sep='\\n')\n",
    "\n",
    "#Load negative reviews train\n",
    "path= url+'train/imdb_train_neg.txt'\n",
    "df_train_neg=pd.read_csv(path,sep='\\n')\n",
    "\n",
    "#Load positive reviews dev\n",
    "path= url+'dev/imdb_dev_pos.txt'\n",
    "df_dev_pos=pd.read_csv(path,sep='\\n')\n",
    "\n",
    "#Load negative reviews dev\n",
    "path= url+'dev/imdb_dev_neg.txt'\n",
    "df_dev_neg=pd.read_csv(path,sep='\\n')\n",
    "\n",
    "#Load positive reviews test\n",
    "path= url+'test/imdb_test_pos.txt'\n",
    "df_test_pos=pd.read_csv(path,sep='\\n')\n",
    "\n",
    "#Load negative reviews test\n",
    "path= url+'test/imdb_test_neg.txt'\n",
    "df_test_neg=pd.read_csv(path,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZxoXc1aeCejO"
   },
   "source": [
    "# Data Pre processing\n",
    "\n",
    "Most of the pre processing will happen when vectorizing. \n",
    "Here we call the stopwords from existing list and lemmatize the reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gn1PzJTwPBwg"
   },
   "outputs": [],
   "source": [
    "#function read stop words in file\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r',) as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    " \n",
    " # load a set of stop words\n",
    "stopwords=get_stop_words(url+'stopwords-en.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmlTdmJLrkAi"
   },
   "outputs": [],
   "source": [
    "# Naming review column for datasets\n",
    "list_df=[df_train_pos,df_train_neg,df_dev_pos,df_dev_neg,df_test_pos,df_test_neg]\n",
    "\n",
    "for df in list_df:\n",
    "    df.columns=['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dCQHBjK7pYE"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmer=WordNetLemmatizer()\n",
    "\n",
    "# lemmatization of words in the reviews\n",
    "for df in list_df:\n",
    "    df['text']=[' '.join([lemmer.lemmatize(word.lower(),'v') for word in text.split(' ')]) for text in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "WDGoPhBY8fmp",
    "outputId": "fe30cc3e-b8d0-4c03-e341-41adb6635628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  well i guess i know the answer to that questio...\n",
      "1  i really like the movie 'the emporer's new gro...\n",
      "2  thats what this movie really takes. a big piec...\n",
      "3  i be look for a documentary of the same journa...\n",
      "4  do anyone care about any of the character in t...\n"
     ]
    }
   ],
   "source": [
    "print(df_train_neg[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CckpESebCYGm"
   },
   "source": [
    "# Validation\n",
    "\n",
    "This is where we will tune the model to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFWq6NfsGND3"
   },
   "outputs": [],
   "source": [
    "# Merging positive and negative reviews into list X_train and adding label into Y_train to train model\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "for instance in df_train_pos['text']:\n",
    "    X_train.append(instance)\n",
    "    Y_train.append(1)\n",
    "for instance in df_train_neg['text']:\n",
    "    X_train.append(instance)\n",
    "    Y_train.append(0)\n",
    "\n",
    "#It is recommended to work with numpy arrays instead of Python lists.\n",
    "X_train_sentanalysis=np.asarray(X_train)\n",
    "Y_train_sentanalysis=np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1VouM5qFLQy"
   },
   "outputs": [],
   "source": [
    "# Merging positive and negative reviews into list X_dev and adding label into Y_train to validate model\n",
    "X_dev=[]\n",
    "Y_dev=[]\n",
    "for instance in df_dev_pos['text']:\n",
    "    X_dev.append(instance)\n",
    "    Y_dev.append(1)\n",
    "for instance in df_dev_neg['text']:\n",
    "    X_dev.append(instance)\n",
    "    Y_dev.append(0)\n",
    "\n",
    "#It is recommended to work with numpy arrays instead of Python lists.\n",
    "X_test=np.asarray(X_dev)\n",
    "Y_test=np.asarray(Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "EF5cKWHymBkB",
    "outputId": "5273fe23-d774-4fc3-d29f-474ee27a80ff"
   },
   "outputs": [],
   "source": [
    "# A test to confirm shape\n",
    "#X_train_sentanalysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhF1hRSWzJL-"
   },
   "outputs": [],
   "source": [
    "# function to train SVM Classifier\n",
    "def train_svm_classifier(X_train_sentanalysis, Y_train_sentanalysis, num_features, kBest):\n",
    "  \n",
    "    # FeatureUnion will apply parallel feature extraction and vectorization to the train set\n",
    "    # Feature extraction selected: Bag of Words, BiGram Bag of words and Tfidf\n",
    "    feature_union = FeatureUnion([\n",
    "        ('bow',CountVectorizer(max_features=num_features, stop_words=stopwords,)),\n",
    "        ('bigram_bow',CountVectorizer(max_features=num_features,ngram_range=(2,2),stop_words=stopwords)),  \n",
    "        ('tfidf', TfidfVectorizer(max_features=num_features, stop_words=stopwords)), \n",
    "    ])\n",
    "\n",
    "    # Pipeline to fit the training set to SVM, starting with feauture extraction and Chi2 feature selection\n",
    "    pipeline = Pipeline([\n",
    "    ('features',feature_union),\n",
    "    ('reducer', SelectKBest(chi2, k=kBest)), \n",
    "    ('classifier', sklearn.svm.SVC(kernel=\"linear\",gamma='auto')),  # train on vectors w/ classifier\n",
    "    ])\n",
    "  \n",
    "    pipeline.fit(X_train_sentanalysis,Y_train_sentanalysis) #fit the data to model\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dp9pgvB7RPEn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 1000 features: 0.83\n",
      "Accuracy with 5000 features: 0.86\n",
      "Accuracy with 10000 features: 0.863\n",
      "Accuracy with 15000 features: 0.863\n",
      "Accuracy with 20000 features: 0.864\n",
      "\n",
      " Best accuracy overall in the dev set is 0.864 with 20000 features.\n"
     ]
    }
   ],
   "source": [
    "list_num_features=[1000,5000,10000,15000,20000] # tried with list 1000,5000,10000,15000, 20000,\n",
    "best_accuracy_dev=0.0\n",
    "for num_features in list_num_features:\n",
    "\n",
    "    # Tested kBest= 5000(acc: 86.1), 3000(acc: 86.7)--Best Result found , 2000(acc: 86.3)\n",
    "    svm=train_svm_classifier(X_train_sentanalysis,Y_train_sentanalysis, num_features, kBest=3000) \n",
    "    predictions = svm.predict(X_test)\n",
    "  \n",
    "    # if a list of number of features is used for validation, the following code will print the Best overall accuracy\n",
    "    accuracy_dev=accuracy_score(Y_test,predictions)\n",
    "    print (\"Accuracy with \"+str(num_features)+\" features: \"+str(round(accuracy_dev,3)))\n",
    "    if accuracy_dev>=best_accuracy_dev:\n",
    "        best_accuracy_dev=accuracy_dev\n",
    "        best_num_features=num_features\n",
    "print (\"\\n Best accuracy overall in the dev set is \"+str(round(best_accuracy_dev,3))+\" with \"+str(best_num_features)+\" features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YO9CfHezgCXM"
   },
   "outputs": [],
   "source": [
    "# A test to confirm number of features\n",
    "# svm.named_steps['classifier'].support_vectors_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XvRwo3Fqrmy9"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gXl1yaA_1MDe"
   },
   "outputs": [],
   "source": [
    "# Merging positive and negative reviews X_test and adding label\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in df_test_pos['text']:\n",
    "    X_test.append(instance)\n",
    "    Y_test.append(1) # 1 for positive\n",
    "for instance in df_test_neg['text']:\n",
    "    X_test.append(instance)\n",
    "    Y_test.append(0) # 0 for negative\n",
    "\n",
    "X_test_sentianalysis=np.asarray(X_test)\n",
    "Y_test_sentianalysis=np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "id": "qcAvhc6RB0sS",
    "outputId": "117bc592-363c-4707-eaea-33cff37d4efd"
   },
   "outputs": [],
   "source": [
    "#training model with tuned parameter\n",
    "svm_clf=train_svm_classifier(X_train_sentanalysis,Y_train_sentanalysis, num_features=20000, kBest=3000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wMv0P9SWrtXR"
   },
   "outputs": [],
   "source": [
    "Predicted_Y_test=svm_clf.predict(X_test_sentianalysis) # Predicting Y_test using the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "mFwbHuZjpkDX",
    "outputId": "efe8da47-8f70-49a8-9f0c-aa86e7c53475",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8565\n",
      "Precision: 0.8561\n",
      "Recall: 0.8571\n",
      "F1-score: 0.8566\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "print('Accuracy: '+str(accuracy_score(Y_test_sentianalysis, Predicted_Y_test).round(4)))\n",
    "print('Precision: '+str(precision_score(Y_test_sentianalysis, Predicted_Y_test).round(4)))\n",
    "print('Recall: '+str(recall_score(Y_test_sentianalysis, Predicted_Y_test).round(4)))\n",
    "print('F1-score: '+str(f1_score(Y_test_sentianalysis, Predicted_Y_test).round(4)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "part2code_1954423.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
